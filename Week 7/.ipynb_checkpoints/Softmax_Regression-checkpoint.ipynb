{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MemoryError: Unable to allocate 26.1 GiB\n",
    "# 메모리 에러 때문에 데이터셋을 40000개로 줄이고, train / test의 비율은 80:20으로 했습니다.\n",
    "X_train, X_test, y_train, y_test = X[:32000], X[32000:40000], Y[:32000], Y[32000:40000]\n",
    "\n",
    "# train / validation / test 각각 60 : 20 : 20\n",
    "X_validation, X_train = X_train[:8000], X_train[8000:]\n",
    "y_validation, y_train = y_train[:8000], y_train[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# validation\n",
    "X_validation = X_validation / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lambd):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    \n",
    "    # 원본 cost function \n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1)) \n",
    "    \n",
    "    # L2 regularization 적용\n",
    "    m = T.shape[1]\n",
    "    cost += (lambd / (2*m)) * (np.sum(np.square(W)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, lambd)\n",
    "        if i % 1000 == 0:\n",
    "            print(cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2841203043910494\n",
      "0.7265323517443094\n",
      "0.5738705527601549\n",
      "0.6272789836192995\n",
      "0.4479204516932398\n",
      "0.6336586629860627\n",
      "0.3318540806543483\n",
      "0.4273903659884456\n",
      "0.2643448672704315\n",
      "0.34573919057826075\n",
      "0.4405870168671068\n",
      "0.598817955861763\n",
      "0.43862288435201824\n",
      "0.4247571480390066\n",
      "0.2549633994257604\n",
      "0.27926420249097117\n",
      "0.4473041817150034\n",
      "0.31446178117212664\n",
      "0.1842005191158812\n",
      "0.5672942612151745\n",
      "0.36540798048471046\n",
      "0.40593534883077653\n",
      "0.33533947424427335\n",
      "0.4497985730282319\n",
      "0.3689269035706434\n",
      "0.4366908891109206\n",
      "0.5505118191322016\n",
      "0.4910361478416822\n",
      "0.4034128581821485\n",
      "0.5954375550220933\n",
      "0.2337924403873565\n",
      "0.3665434002573862\n",
      "0.2283205508067881\n",
      "0.3286272392952185\n",
      "0.4395247554819194\n",
      "0.5360652564070197\n",
      "0.3687234437070499\n",
      "0.40203235263343207\n",
      "0.24936259870054622\n",
      "0.262734643726269\n",
      "0.45710718430598507\n",
      "0.2624584179243252\n",
      "0.19289970662537537\n",
      "0.5801302468538696\n",
      "0.3517729740674669\n",
      "0.3763822740508503\n",
      "0.3118911087330379\n",
      "0.45791990858037734\n",
      "0.35175424958043705\n",
      "0.43544004873972236\n",
      "lambd: 0.01, accuracy score: 0.917\n",
      "2.279629320567332\n",
      "0.7900276944941933\n",
      "0.4540244631096256\n",
      "0.4270937784917742\n",
      "0.5465365776405446\n",
      "0.4447606792985309\n",
      "0.5307762918964103\n",
      "0.5826643244659996\n",
      "0.635529347729012\n",
      "0.6625836407091579\n",
      "0.5067583737957833\n",
      "0.7724480411825321\n",
      "0.5670657263316841\n",
      "0.4934408354786413\n",
      "0.7070176859778796\n",
      "0.7327704689941175\n",
      "0.5865555118933147\n",
      "0.7529349556306257\n",
      "0.655950339112908\n",
      "0.5733413809899233\n",
      "0.6560514298386775\n",
      "0.8556964328443868\n",
      "0.8621698857290532\n",
      "0.6162860663678716\n",
      "0.693337221517079\n",
      "1.0038281854481843\n",
      "0.6422728542184266\n",
      "0.6334429360071843\n",
      "0.7367201057991973\n",
      "0.6284155395704945\n",
      "0.7454111731209936\n",
      "0.7869545559369799\n",
      "0.8030755700784287\n",
      "0.8419755635506004\n",
      "0.7105162903347514\n",
      "0.9543963304644203\n",
      "0.7803889166640992\n",
      "0.690510763316174\n",
      "0.8870980028387727\n",
      "0.8903890777119177\n",
      "0.7464963517811473\n",
      "0.9169776789928239\n",
      "0.8214109205340916\n",
      "0.7295686913993487\n",
      "0.8584209587845897\n",
      "0.9836168543423197\n",
      "1.0121566428515099\n",
      "0.7963342385942731\n",
      "0.8655203843500302\n",
      "1.1924041461316768\n",
      "lambd: 0.1, accuracy score: 0.914\n",
      "2.28221438494863\n",
      "0.5766539423964047\n",
      "0.35595113789566113\n",
      "0.5589593500549379\n",
      "0.4352149860712742\n",
      "0.3737975623121295\n",
      "0.2947022061433474\n",
      "0.4115046538164453\n",
      "0.34843989849254786\n",
      "0.23500374579121774\n",
      "0.3852427034067504\n",
      "0.3769340976209504\n",
      "0.3950713624626727\n",
      "0.3612884769047971\n",
      "0.23278232273654328\n",
      "0.17622320563704402\n",
      "0.26539420690692633\n",
      "0.35775055374839576\n",
      "0.19180240833623646\n",
      "0.3258600687723692\n",
      "0.3073866110272043\n",
      "0.2171407204050488\n",
      "0.22135650818990651\n",
      "0.2126276823035521\n",
      "0.25909912254589434\n",
      "0.18373766604410124\n",
      "0.2355567871322999\n",
      "0.3919242345480655\n",
      "0.31025171661702017\n",
      "0.278286083926293\n",
      "0.20603386296468335\n",
      "0.34650318299499416\n",
      "0.31966458525675445\n",
      "0.2139560339584063\n",
      "0.30854581485344645\n",
      "0.3527289166068204\n",
      "0.3865039431109128\n",
      "0.30526244080352877\n",
      "0.19718192256531447\n",
      "0.12763572887196994\n",
      "0.22866414150763242\n",
      "0.3205183014590868\n",
      "0.15591716829073526\n",
      "0.29662084310820425\n",
      "0.28998691997663484\n",
      "0.1920013227541904\n",
      "0.1769155234663751\n",
      "0.16978800330619792\n",
      "0.22502066999298276\n",
      "0.16035730742434848\n",
      "lambd: 0, accuracy score: 0.9095\n",
      "2.2811941423690016\n",
      "1.1189247623739746\n",
      "1.4254730388335202\n",
      "1.7359138764411206\n",
      "1.9712561335924461\n",
      "2.2834281641797394\n",
      "2.715202025737249\n",
      "2.7644431275433052\n",
      "2.68887075923268\n",
      "2.894926875082315\n",
      "3.098746862189108\n",
      "3.356884648222287\n",
      "3.3037579931527774\n",
      "3.450205271724106\n",
      "3.6373163639556854\n",
      "3.728897212130283\n",
      "3.7938753696873393\n",
      "3.8398301196106925\n",
      "4.034468883458666\n",
      "3.9924921399378173\n",
      "4.155545147050649\n",
      "4.242902931525231\n",
      "4.382068925775415\n",
      "4.437520973152854\n",
      "4.496243313164305\n",
      "4.562595522174597\n",
      "4.6758436792745055\n",
      "4.833413014669368\n",
      "4.937803884257572\n",
      "5.183443785548215\n",
      "5.430396662139945\n",
      "5.406512144009315\n",
      "5.29923362841355\n",
      "5.4133124085231765\n",
      "5.556983751821127\n",
      "5.7445467408063156\n",
      "5.629333992036233\n",
      "5.717991473290267\n",
      "5.878198840689449\n",
      "5.892300280088514\n",
      "5.960118185335812\n",
      "5.9529558074735185\n",
      "6.155150809782287\n",
      "6.068517180299177\n",
      "6.1950582493489526\n",
      "6.240289215547777\n",
      "6.349537786005856\n",
      "6.375371976857678\n",
      "6.421391041122087\n",
      "6.47449859287796\n",
      "lambd: 1, accuracy score: 0.91875\n",
      "2.2790855458166672\n",
      "6.551159890391929\n",
      "10.80115896404497\n",
      "13.963745652112943\n",
      "16.254704340897593\n",
      "18.68881257208662\n",
      "21.063632223028197\n",
      "23.084434700677544\n",
      "24.854106653988342\n",
      "26.30532143139999\n",
      "28.02658554576841\n",
      "29.422102104292446\n",
      "30.7569615184097\n",
      "32.18833707881007\n",
      "33.46033863632883\n",
      "34.467434135249036\n",
      "35.889664840308235\n",
      "36.98250026177578\n",
      "38.40663633048719\n",
      "39.205586814252534\n",
      "40.15546765988915\n",
      "41.42458030540569\n",
      "42.61212878458962\n",
      "43.729876427339825\n",
      "44.05586006854712\n",
      "45.13093028590478\n",
      "46.08578471037592\n",
      "46.952691044893285\n",
      "47.503967131796855\n",
      "48.41155869096217\n",
      "49.49069325784285\n",
      "50.5658933352978\n",
      "51.37384164247959\n",
      "52.21268114922905\n",
      "53.23105788638349\n",
      "53.96646650858579\n",
      "54.73820492988763\n",
      "55.61903916032009\n",
      "56.40896673144722\n",
      "56.97079303527642\n",
      "57.91769349190057\n",
      "58.69532500224252\n",
      "59.76934834760359\n",
      "60.24545946267359\n",
      "60.950670906237924\n",
      "61.98116755229533\n",
      "62.8735504547563\n",
      "63.77254706504558\n",
      "63.93005773226768\n",
      "64.76426203320024\n",
      "lambd: 10, accuracy score: 0.91175\n",
      "2.2828817372083576\n",
      "61.53718647659884\n",
      "103.85537702929379\n",
      "133.52959343471036\n",
      "161.5535083909295\n",
      "186.04268996635514\n",
      "206.69695774658166\n",
      "225.85971194010324\n",
      "242.0999883523988\n",
      "259.41748618310635\n",
      "276.761874527386\n",
      "290.36700570474255\n",
      "305.7048646501556\n",
      "318.5008987799701\n",
      "329.68395970047385\n",
      "339.67620630522646\n",
      "352.67716577714674\n",
      "365.8968017517516\n",
      "373.35097672299224\n",
      "386.8369446208822\n",
      "396.3432755982327\n",
      "407.39527548770604\n",
      "415.52495613518124\n",
      "427.3135000370932\n",
      "437.4353409841425\n",
      "445.7114769935767\n",
      "456.3795855706561\n",
      "462.92349279178376\n",
      "472.8451371901187\n",
      "482.50900875495836\n",
      "491.5297118666152\n",
      "500.7045193697827\n",
      "508.3994632791223\n",
      "517.9359920906548\n",
      "529.4083798454774\n",
      "536.9157971094027\n",
      "546.3963153308307\n",
      "553.8628530921137\n",
      "560.2635795741747\n",
      "566.131047646569\n",
      "575.2989078338437\n",
      "584.325790054566\n",
      "588.0892293636992\n",
      "598.1283333500553\n",
      "604.9119941686931\n",
      "613.2470869716195\n",
      "618.5455322119674\n",
      "627.9402286945342\n",
      "635.5649798630111\n",
      "641.5624963562038\n",
      "lambd: 100, accuracy score: 0.913875\n",
      "{0.01: 0.917, 0.1: 0.914, 0: 0.9095, 1: 0.91875, 10: 0.91175, 100: 0.913875}\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "lambds = [0.01, 0.1, 0, 1, 10, 100]\n",
    "lambds_dict = {}\n",
    "for lambd in lambds:\n",
    "    initial_cost = compute_cost(X, T, W, lambd)\n",
    "    # print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "    (cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)\n",
    "    \n",
    "    # Validation set에 대한 Accuracy\n",
    "    X_ = np.hstack((np.ones((np.size(X_test, 0),1)), X_validation))\n",
    "    T_ = y_validation\n",
    "    y_pred = predict(X_, W_optimal)\n",
    "    score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_validation))\n",
    "    \n",
    "    lambds_dict[lambd] = score\n",
    "    print(f'lambd: {lambd}, accuracy score: {score}')\n",
    "\n",
    "print(lambds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal lambd: 1\n"
     ]
    }
   ],
   "source": [
    "max_score = max(lambds_dict.values())\n",
    "for key, val in lambds_dict.items():\n",
    "    if val == max_score:\n",
    "        lambd = key\n",
    "        print(f'optimal lambd: {lambd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905625\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1))) / float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
