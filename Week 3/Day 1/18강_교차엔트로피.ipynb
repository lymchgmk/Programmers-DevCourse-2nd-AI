{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18강. 교차엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엔트로피\n",
    "    - 자기정보\n",
    "    - 엔트로피\n",
    "- 교차엔트로피\n",
    "    - 교차엔트로피의 정의\n",
    "    - 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)-1. 자기정보(Self-information): $i(A)$\n",
    "   - $A$: 사건\n",
    "   - $i(A) = log_{b} \\left ( \\frac {1} {P(A)} \\right ) = -log_{b}P(A)$\n",
    "       - 확률이 높은 사건: 정보가 많지 않음.\n",
    "       \n",
    "       \n",
    "   - 정보의 단위\n",
    "       - $b = 2$: bits\n",
    "       - $b = e$: nats\n",
    "       - $b = 10$: hartleys\n",
    "       \n",
    "       \n",
    "   - 특성\n",
    "       - $i(AB) = log_{b} \\left ( \\frac {1} {P(A)P(B)} \\right ) = log_{b} \\left ( \\frac {1} {P(A)} \\right ) + log_{b} \\left ( \\frac {1} {P(B)} \\right ) =i(A) + i(B)$\n",
    "       - $P(H) = \\frac {1} {8}, P(T) = \\frac {7} {8}$\n",
    "           - $i(H)$ = 3비트, $i(T)$ = 0.193비트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)-2. 엔트로피(entropy)\n",
    "> 자기 정보의 평균\n",
    "$$H(X) = \\sum_{j}P(A_{j})i(A_{j}) = -\\sum_{j}P(A_{j})log_{2}P(A_{j})$$\n",
    "\n",
    "- 특성\n",
    "   - $0 \\leq H(X) \\leq log_{2} K$\n",
    "       - $K$: 사건의 수\n",
    "       \n",
    "       \n",
    "- 엔트로피\n",
    "    1. 엔트로피의 활용\n",
    "        - 평균비트수를 표현\n",
    "        - 데이터 압축에 사용 가능\n",
    "        \n",
    "    2. 4가지 정보를 표현하는데 필요한 비트 수\n",
    "        - 일반적으로 2비트\n",
    "    3. $i(X)$를 활용하는 경우\n",
    "        - 평균비트수\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 교차엔트로피\n",
    "- 확률분포 $P$와 $Q$\n",
    "    - $S = {A_{j}}$\n",
    "        - $P(A_{j})$: 확률분포 $P$에서 사건 $A_{j}$가 발생할 확률\n",
    "        - $Q(A_{j})$: 확률분포 $Q$에서 사건 $A_{j}$가 발생할 확률\n",
    "        - $i(A_{j})$: 확률분포 $Q$에서 사건 $A_{j}$의 자기정보\n",
    "            - $i(A_{j}) = -log_{2}Q(A_{j})$\n",
    "            - 자기 정보는 $A_{j}$를 표현하는 비트 수\n",
    "            - 잘못된 확률분포 $Q$를 사용하게 되면, 실제 최적의 비트 수를 사용하지 못하게 됨.\n",
    "\n",
    "\n",
    "- 교차엔트로피 $H(P, Q)$\n",
    "> 집합 $S$상에서 확률분포 $P$에 대한 확률분포 $Q$의 교차 엔트로피\n",
    "\n",
    "    - 확률분포 $P$에서 $i(A_{j})$의 평균\n",
    "        - $H(P, Q) = \\sum_{j}P(A_{j})i(A_{j}) = -\\sum_{j}P(A_{j})log_{2}Q(A_{j}) = -\\sum_{x \\in X}P(x)log_{2}Q(x)$\n",
    "        - 이 값은 정확한 확률분포 $P$를 사용했을 때의 비트 수보다 크게 됨\n",
    "            - $H(P, Q) = -\\sum_{x \\in X}P(x)log_{2}Q(x) \\geq -\\sum_{x \\in X}P(x)log_{2}P(x) = H(P)$\n",
    "        - 따라서, 이 값은 **$P$와 $Q$가 얼마나 비슷한지**를 표현\n",
    "            - 같으면, $H(P, Q) = H(P)$ \n",
    "            - 다르면, $H(P, Q) > H(P)$\n",
    "\n",
    "\n",
    "- 분류 문제에서의 손실함수\n",
    "    - 분류문제\n",
    "    > 주어진 대상이 A인지 아닌지를 판단.\n",
    "    > 주어진 대상이 A, B, C, ... 중 어느 것인지를 판단\n",
    "    \n",
    "    - 기계학습에서는 주어진 대상이 각 그룹에 속할 확률을 제공\n",
    "    \n",
    "    - 원하는 답 P와 제시된 답 Q가 얼마나 다른지에 대한 척도 필요\n",
    "\n",
    "\n",
    "- 손실함수의 예\n",
    "    - 제곱합: $\\sum(p_{i} - q_{i})^2$\n",
    "        - 확률이 다를수록 큰 값을 가짐\n",
    "        - 하지만 학습 속도 느림\n",
    "    - **교차 엔트로피**: $H(P, Q)$\n",
    "        - 확률이 다를수록 큰 값을 가짐\n",
    "        - **제곱합 보다 학습 속도 빠름**\n",
    "        - 분류 문제에서 주로 교차 엔트로피 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교차엔트로피 예제, $S = \\{A, B\\}$\n",
    "    - 실제 상황\n",
    "        - $P = [1, 0]$\n",
    "            - $P(A) = 1, P(B) = 0$\n",
    "        \n",
    "    - 예측 $Q(X)$\n",
    "        - $[0.8, 0.2]: Q(A) = 0.8, Q(B) = 0.2$\n",
    "            - $H(P, Q) = -\\sum_{x \\in X} P(x)log_{2}Q(x) = -1 \\times log_{2} 0.8 = 0.3219$\n",
    "        - $[0.5, 0.5]: Q(A) = 0.5, Q(B) = 0.5$\n",
    "            - $H(P, Q) = -\\sum_{x \\in X} P(x)log_{2}Q(x) = -1 \\times log_{2} 0.5 = 1$\n",
    "        - $[0.2, 0.8]: Q(A) = 0.2, Q(B) = 0.8$\n",
    "            - $H(P, Q) = -\\sum_{x \\in X} P(x)log_{2}Q(x) = -1 \\times log_{2} 0.2 = 2.32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3219280948873623\n",
      "1.0\n",
      "2.321928094887362\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def crossentropy(P, Q):\n",
    "    return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])\n",
    "\n",
    "P = [1, 0]\n",
    "\n",
    "Q = [0.8, 0.2]\n",
    "print(crossentropy(P, Q))\n",
    "\n",
    "Q = [0.5, 0.5]\n",
    "print(crossentropy(P, Q))\n",
    "\n",
    "Q = [0.2, 0.8]\n",
    "print(crossentropy(P, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5145731728297583\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def crossentropy(P, Q):\n",
    "    return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])\n",
    "\n",
    "P =[1, 0, 0, 0]\n",
    "Q = [0.7, 0.1, 0.1, 0.1]\n",
    "print(crossentropy(P, Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
